name: Comprehensive Test Suite & Quality Assurance

# Trigger comprehensive testing on pull requests and main branch commits
# Implements Section 4.1.1.5 test execution workflow requirements
on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  # Enable manual workflow dispatch for testing
  workflow_dispatch:
    inputs:
      coverage_threshold:
        description: 'Coverage threshold percentage'
        required: false
        default: '90'
        type: string
      run_performance_tests:
        description: 'Run performance benchmark tests'
        required: false
        default: true
        type: boolean

# Global environment variables for consistent test execution
env:
  COVERAGE_THRESHOLD: ${{ inputs.coverage_threshold || '90' }}
  CRITICAL_MODULE_COVERAGE_THRESHOLD: '100'
  PYTHONUNBUFFERED: '1'
  PYTEST_TIMEOUT: '30'
  BENCHMARK_TIMEOUT: '120'
  
# Concurrency control to prevent redundant test runs
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  # Pre-flight quality checks
  code-quality:
    name: Code Quality & Security Checks
    runs-on: ubuntu-latest
    timeout-minutes: 10
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Full history for comprehensive analysis
        
    - name: Set up Python 3.11 for quality checks
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'
        cache: 'pip'
        
    - name: Install quality checking tools
      run: |
        python -m pip install --upgrade pip
        pip install black==24.3.0 isort==5.12.0 flake8==7.0.0 mypy==1.8.0
        pip install flake8-docstrings flake8-bugbear bandit safety
        
    - name: Code formatting validation with Black
      run: |
        echo "::group::Black Code Formatting Check"
        black --check --diff --color src/ tests/
        echo "::endgroup::"
        
    - name: Import sorting validation with isort
      run: |
        echo "::group::Import Sorting Validation"
        isort --check-only --diff --color src/ tests/
        echo "::endgroup::"
        
    - name: Linting analysis with flake8
      run: |
        echo "::group::Flake8 Linting Analysis"
        flake8 src/ tests/ --max-line-length=88 --extend-ignore=E203,W503 --statistics
        echo "::endgroup::"
        
    - name: Type checking with mypy
      run: |
        echo "::group::MyPy Type Checking"
        # Install minimal dependencies for type checking
        pip install loguru pydantic numpy pandas types-PyYAML
        mypy src/flyrigloader --ignore-missing-imports --no-strict-optional
        echo "::endgroup::"
        
    - name: Security analysis with bandit
      run: |
        echo "::group::Security Analysis"
        bandit -r src/ -f json -o bandit-report.json || true
        bandit -r src/ --severity-level medium
        echo "::endgroup::"
        
    - name: Dependency security check with safety
      run: |
        echo "::group::Dependency Security Check"
        safety check --json --output safety-report.json || true
        safety check
        echo "::endgroup::"
        
    - name: Upload security reports
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: security-reports
        path: |
          bandit-report.json
          safety-report.json
        retention-days: 30

  # Comprehensive test matrix across Python versions and platforms
  test-matrix:
    name: Test Suite (Python ${{ matrix.python-version }} on ${{ matrix.os }})
    runs-on: ${{ matrix.os }}
    timeout-minutes: 45
    needs: code-quality
    
    strategy:
      fail-fast: false  # Continue testing other combinations on failure
      matrix:
        os: [ubuntu-latest, windows-latest, macos-latest]
        python-version: ['3.8', '3.9', '3.10', '3.11']
        # Optimize test execution by reducing matrix on non-primary platforms
        exclude:
          - os: windows-latest
            python-version: '3.8'
          - os: windows-latest 
            python-version: '3.9'
          - os: macos-latest
            python-version: '3.8'
          - os: macos-latest
            python-version: '3.9'
    
    steps:
    - name: Checkout repository with full history
      uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Required for accurate coverage reporting
        
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v5
      with:
        python-version: ${{ matrix.python-version }}
        cache: 'pip'
        cache-dependency-path: |
          pyproject.toml
          requirements*.txt
          
    - name: Configure Git for Windows (if needed)
      if: runner.os == 'Windows'
      run: |
        git config --global core.autocrlf false
        git config --global core.eol lf
        
    - name: Install system dependencies (Ubuntu)
      if: runner.os == 'Linux'
      run: |
        sudo apt-get update
        sudo apt-get install -y build-essential
        
    - name: Install system dependencies (macOS)
      if: runner.os == 'macOS'
      run: |
        brew install --quiet gcc
        
    - name: Upgrade pip and install build tools
      run: |
        python -m pip install --upgrade pip setuptools wheel
        
    - name: Install package with dev dependencies
      run: |
        echo "::group::Install Package Dependencies"
        # Install package in editable mode with development dependencies
        pip install -e .[dev]
        echo "::endgroup::"
        
    - name: Install additional testing infrastructure
      run: |
        echo "::group::Install Testing Infrastructure"
        # Ensure all testing dependencies are installed with exact versions
        pip install pytest>=7.0.0 pytest-cov>=6.1.1 pytest-mock>=3.14.1
        pip install pytest-benchmark>=4.0.0 coverage>=7.8.2 hypothesis>=6.131.9
        pip install pytest-xdist>=3.7.0 pytest-timeout>=2.3.0
        pip install pytest-html>=4.0.0 pytest-json-report>=1.5.0
        echo "::endgroup::"
        
    - name: Verify installation and environment
      run: |
        echo "::group::Environment Verification"
        python --version
        pip --version
        pytest --version
        coverage --version
        python -c "import flyrigloader; print(f'flyrigloader imported successfully')"
        python -c "import numpy, pandas, loguru, pydantic; print('All core dependencies available')"
        echo "::endgroup::"
        
    - name: Run unit tests with coverage
      run: |
        echo "::group::Unit Test Execution"
        pytest tests/ \
          --cov=src/flyrigloader \
          --cov-report=xml:coverage-${{ matrix.python-version }}-${{ matrix.os }}.xml \
          --cov-report=html:htmlcov-${{ matrix.python-version }}-${{ matrix.os }} \
          --cov-report=term-missing \
          --cov-fail-under=${{ env.COVERAGE_THRESHOLD }} \
          --cov-branch \
          --cov-context=test \
          --junit-xml=test-results-${{ matrix.python-version }}-${{ matrix.os }}.xml \
          --html=test-report-${{ matrix.python-version }}-${{ matrix.os }}.html \
          --self-contained-html \
          --json-report --json-report-file=test-report-${{ matrix.python-version }}-${{ matrix.os }}.json \
          --timeout=${{ env.PYTEST_TIMEOUT }} \
          --timeout-method=thread \
          -n auto \
          --dist=worksteal \
          --durations=10 \
          --strict-markers \
          --strict-config \
          -v \
          -x \
          --tb=short
        echo "::endgroup::"
        
    - name: Run performance benchmarks (conditional)
      if: ${{ inputs.run_performance_tests != false && matrix.python-version == '3.11' && matrix.os == 'ubuntu-latest' }}
      run: |
        echo "::group::Performance Benchmark Execution"
        # Run performance benchmarks with SLA validation
        pytest tests/ \
          --benchmark-only \
          --benchmark-min-rounds=5 \
          --benchmark-disable-gc \
          --benchmark-warmup=on \
          --benchmark-sort=mean \
          --benchmark-json=benchmark-results-${{ matrix.python-version }}-${{ matrix.os }}.json \
          --benchmark-histogram=benchmark-histogram-${{ matrix.python-version }}-${{ matrix.os }}.svg \
          --timeout=${{ env.BENCHMARK_TIMEOUT }} \
          -m benchmark \
          --tb=short \
          -v
        echo "::endgroup::"
        
    - name: Validate critical module coverage
      run: |
        echo "::group::Critical Module Coverage Validation"
        # Check critical module coverage (data loading and validation)
        coverage report --include="*/flyrigloader/io/*" --fail-under=${{ env.CRITICAL_MODULE_COVERAGE_THRESHOLD }}
        coverage report --include="*/flyrigloader/config/*" --fail-under=${{ env.CRITICAL_MODULE_COVERAGE_THRESHOLD }}
        coverage report --include="*/flyrigloader/discovery/*" --fail-under=95  # Slightly relaxed for discovery
        echo "::endgroup::"
        
    - name: Generate detailed coverage analysis
      if: always()
      run: |
        echo "::group::Coverage Analysis Generation"
        # Generate comprehensive coverage reports
        coverage report --show-missing --precision=2
        coverage html -d htmlcov-detailed-${{ matrix.python-version }}-${{ matrix.os }}
        coverage json -o coverage-detailed-${{ matrix.python-version }}-${{ matrix.os }}.json
        echo "::endgroup::"
        
    - name: Upload test results and coverage artifacts
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: test-results-${{ matrix.python-version }}-${{ matrix.os }}
        path: |
          coverage-*.xml
          coverage-*.json
          htmlcov-*
          test-results-*.xml
          test-report-*.html
          test-report-*.json
          benchmark-results-*.json
          benchmark-histogram-*.svg
        retention-days: 30
        
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v4
      if: always()
      with:
        file: coverage-${{ matrix.python-version }}-${{ matrix.os }}.xml
        flags: unittests,${{ matrix.python-version }},${{ matrix.os }}
        name: codecov-${{ matrix.python-version }}-${{ matrix.os }}
        fail_ci_if_error: true
        verbose: true
        token: ${{ secrets.CODECOV_TOKEN }}

  # Integration and end-to-end testing
  integration-tests:
    name: Integration & End-to-End Tests
    runs-on: ubuntu-latest
    timeout-minutes: 30
    needs: code-quality
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Set up Python 3.11 for integration tests
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'
        cache: 'pip'
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e .[dev]
        
    - name: Run integration tests
      run: |
        echo "::group::Integration Test Suite"
        pytest tests/ \
          -m "integration" \
          --cov=src/flyrigloader \
          --cov-report=xml:coverage-integration.xml \
          --cov-report=html:htmlcov-integration \
          --cov-report=term-missing \
          --timeout=60 \
          --junit-xml=integration-test-results.xml \
          --html=integration-test-report.html \
          --self-contained-html \
          -v \
          --tb=long
        echo "::endgroup::"
        
    - name: Run end-to-end workflow tests
      run: |
        echo "::group::End-to-End Workflow Tests"
        pytest tests/ \
          -k "test_complete_workflow or test_end_to_end" \
          --timeout=90 \
          --junit-xml=e2e-test-results.xml \
          -v \
          --tb=long
        echo "::endgroup::"
        
    - name: Upload integration test artifacts
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: integration-test-results
        path: |
          coverage-integration.xml
          htmlcov-integration/
          integration-test-results.xml
          integration-test-report.html
          e2e-test-results.xml
        retention-days: 30

  # Performance regression testing
  performance-validation:
    name: Performance SLA Validation
    runs-on: ubuntu-latest
    timeout-minutes: 20
    needs: code-quality
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Set up Python 3.11 for performance testing
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'
        cache: 'pip'
        
    - name: Install dependencies with performance profiling tools
      run: |
        python -m pip install --upgrade pip
        pip install -e .[dev]
        pip install memory-profiler psutil
        
    - name: Run data loading SLA validation
      run: |
        echo "::group::Data Loading SLA Validation (1s per 100MB)"
        pytest tests/ \
          -k "test_data_loading_performance" \
          --benchmark-only \
          --benchmark-min-rounds=3 \
          --benchmark-max-time=60 \
          --benchmark-disable-gc \
          --benchmark-json=data-loading-benchmarks.json \
          -m "benchmark" \
          --timeout=120 \
          -v
        echo "::endgroup::"
        
    - name: Run DataFrame transformation SLA validation
      run: |
        echo "::group::DataFrame Transformation SLA Validation (500ms per 1M rows)"
        pytest tests/ \
          -k "test_transformation_performance" \
          --benchmark-only \
          --benchmark-min-rounds=3 \
          --benchmark-max-time=30 \
          --benchmark-disable-gc \
          --benchmark-json=transformation-benchmarks.json \
          -m "benchmark" \
          --timeout=60 \
          -v
        echo "::endgroup::"
        
    - name: Validate performance against SLA thresholds
      run: |
        echo "::group::SLA Threshold Validation"
        python -c "
        import json
        import sys
        
        # Load benchmark results
        try:
            with open('data-loading-benchmarks.json') as f:
                data_loading = json.load(f)
            with open('transformation-benchmarks.json') as f:
                transformation = json.load(f)
        except FileNotFoundError:
            print('Benchmark files not found - performance tests may have been skipped')
            sys.exit(0)
        
        # Validate data loading SLA (1s per 100MB)
        for benchmark in data_loading.get('benchmarks', []):
            mean_time = benchmark['stats']['mean']
            data_size_mb = benchmark.get('params', {}).get('data_size_mb', 100)
            sla_time = data_size_mb * 0.01  # 1s per 100MB = 0.01s per MB
            
            print(f'Data loading: {mean_time:.3f}s for {data_size_mb}MB (SLA: {sla_time:.3f}s)')
            if mean_time > sla_time:
                print(f'❌ SLA VIOLATION: Data loading took {mean_time:.3f}s, expected ≤ {sla_time:.3f}s')
                sys.exit(1)
            else:
                print(f'✅ SLA MET: Data loading within threshold')
        
        # Validate transformation SLA (500ms per 1M rows)
        for benchmark in transformation.get('benchmarks', []):
            mean_time = benchmark['stats']['mean']
            row_count = benchmark.get('params', {}).get('row_count', 1000000)
            sla_time = (row_count / 1000000) * 0.5  # 500ms per 1M rows
            
            print(f'Transformation: {mean_time:.3f}s for {row_count} rows (SLA: {sla_time:.3f}s)')
            if mean_time > sla_time:
                print(f'❌ SLA VIOLATION: Transformation took {mean_time:.3f}s, expected ≤ {sla_time:.3f}s')
                sys.exit(1)
            else:
                print(f'✅ SLA MET: Transformation within threshold')
        
        print('🎉 All performance SLAs validated successfully!')
        "
        echo "::endgroup::"
        
    - name: Upload performance validation artifacts
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: performance-validation-results
        path: |
          *-benchmarks.json
        retention-days: 30

  # Final quality gate validation
  quality-gate:
    name: Quality Gate Validation
    runs-on: ubuntu-latest
    timeout-minutes: 10
    needs: [test-matrix, integration-tests, performance-validation]
    if: always()
    
    steps:
    - name: Download all test artifacts
      uses: actions/download-artifact@v4
      with:
        path: test-artifacts/
        
    - name: Validate overall test results
      run: |
        echo "::group::Test Result Validation"
        
        # Initialize result tracking
        overall_success=true
        
        # Check for test result files
        test_files=$(find test-artifacts/ -name "test-results-*.xml" | wc -l)
        echo "Found $test_files test result files"
        
        if [ $test_files -eq 0 ]; then
          echo "❌ No test results found"
          overall_success=false
        fi
        
        # Check for coverage files
        coverage_files=$(find test-artifacts/ -name "coverage-*.xml" | wc -l)
        echo "Found $coverage_files coverage files"
        
        if [ $coverage_files -eq 0 ]; then
          echo "❌ No coverage results found"
          overall_success=false
        fi
        
        # Validate artifact completeness
        echo "📊 Test execution summary:"
        find test-artifacts/ -name "*.xml" -o -name "*.json" -o -name "*.html" | sort
        
        if [ "$overall_success" = true ]; then
          echo "✅ Quality gate validation passed"
        else
          echo "❌ Quality gate validation failed"
          exit 1
        fi
        echo "::endgroup::"
        
    - name: Generate consolidated test report
      run: |
        echo "::group::Consolidated Test Report"
        
        echo "# Test Execution Summary" > test-summary.md
        echo "" >> test-summary.md
        echo "## Environment Matrix" >> test-summary.md
        echo "- **Python Versions:** 3.8, 3.9, 3.10, 3.11" >> test-summary.md
        echo "- **Platforms:** Ubuntu, Windows, macOS" >> test-summary.md
        echo "- **Coverage Threshold:** ≥${{ env.COVERAGE_THRESHOLD }}%" >> test-summary.md
        echo "- **Critical Module Coverage:** ${{ env.CRITICAL_MODULE_COVERAGE_THRESHOLD }}%" >> test-summary.md
        echo "" >> test-summary.md
        
        echo "## Quality Gates" >> test-summary.md
        echo "- ✅ Code quality checks passed" >> test-summary.md
        echo "- ✅ Unit test execution completed" >> test-summary.md
        echo "- ✅ Integration test validation passed" >> test-summary.md
        echo "- ✅ Performance SLA validation completed" >> test-summary.md
        echo "- ✅ Coverage thresholds enforced" >> test-summary.md
        echo "" >> test-summary.md
        
        echo "## Artifacts Generated" >> test-summary.md
        echo "- Test results (JUnit XML)" >> test-summary.md
        echo "- Coverage reports (XML, HTML, JSON)" >> test-summary.md
        echo "- Performance benchmarks (JSON)" >> test-summary.md
        echo "- Security analysis reports" >> test-summary.md
        
        cat test-summary.md
        echo "::endgroup::"
        
    - name: Upload consolidated test summary
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: test-execution-summary
        path: test-summary.md
        retention-days: 90

  # Notification and reporting
  notify-results:
    name: Test Results Notification
    runs-on: ubuntu-latest
    timeout-minutes: 5
    needs: [quality-gate]
    if: always() && github.event_name == 'pull_request'
    
    steps:
    - name: Generate test status comment
      uses: actions/github-script@v7
      with:
        script: |
          const { data: workflow } = await github.rest.actions.getWorkflowRun({
            owner: context.repo.owner,
            repo: context.repo.repo,
            run_id: context.runId
          });
          
          const conclusion = workflow.conclusion;
          const status = workflow.status;
          
          let emoji = '✅';
          let statusText = 'All tests passed';
          
          if (conclusion === 'failure') {
            emoji = '❌';
            statusText = 'Some tests failed';
          } else if (conclusion === 'cancelled') {
            emoji = '⏹️';
            statusText = 'Tests were cancelled';
          } else if (status === 'in_progress') {
            emoji = '🔄';
            statusText = 'Tests are running';
          }
          
          const body = `
          ## ${emoji} Test Suite Results
          
          **Status:** ${statusText}
          **Conclusion:** ${conclusion}
          **Workflow:** [${workflow.name}](${workflow.html_url})
          
          ### Quality Gates Status
          - Code Quality & Security: ${conclusion === 'success' ? '✅' : '❌'}
          - Multi-Platform Tests: ${conclusion === 'success' ? '✅' : '❌'}
          - Integration Tests: ${conclusion === 'success' ? '✅' : '❌'}
          - Performance Validation: ${conclusion === 'success' ? '✅' : '❌'}
          
          ### Coverage Requirements
          - Overall Coverage: ≥90%
          - Critical Modules: 100%
          - Branch Coverage: Enabled
          
          View detailed test results and artifacts in the [workflow run](${workflow.html_url}).
          `;
          
          // Find existing comment
          const { data: comments } = await github.rest.issues.listComments({
            owner: context.repo.owner,
            repo: context.repo.repo,
            issue_number: context.issue.number
          });
          
          const existingComment = comments.find(comment => 
            comment.body.includes('Test Suite Results') && 
            comment.user.type === 'Bot'
          );
          
          if (existingComment) {
            await github.rest.issues.updateComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              comment_id: existingComment.id,
              body: body
            });
          } else {
            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
              body: body
            });
          }